{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import csv\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName(\"Employess\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Csv File with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark=spark.read.csv('employee_data.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: double (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(Name='surend', Age=20, Experience=2, Salary=20000.0, Address='Tn')"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "df_spark.printSchema()\n",
    "df_spark.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+-------+-------+\n",
      "|  Name| Age|Experience| Salary|Address|\n",
      "+------+----+----------+-------+-------+\n",
      "|surend|  20|         2|20000.0|     Tn|\n",
      "| kumar|  21|         1|30000.0|     Kl|\n",
      "|  hari|  23|         3|40000.0|     Tn|\n",
      "|sudhan|  20|         5|   NULL|     Kr|\n",
      "|  alex|NULL|         7|30000.0|     kl|\n",
      "|   bob|  25|      NULL|10000.0|   NULL|\n",
      "|  jhon|NULL|      NULL|   NULL|   NULL|\n",
      "+------+----+----------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------------+------------------+-----------------+-------+\n",
      "|summary|  Name|             Age|        Experience|           Salary|Address|\n",
      "+-------+------+----------------+------------------+-----------------+-------+\n",
      "|  count|     7|               5|                 5|                5|      5|\n",
      "|   mean|  NULL|            21.8|               3.6|          26000.0|   NULL|\n",
      "| stddev|  NULL|2.16794833886788|2.4083189157584592|11401.75425099138|   NULL|\n",
      "|    min|  alex|              20|                 1|          10000.0|     Kl|\n",
      "|    max|surend|              25|                 7|          40000.0|     kl|\n",
      "+-------+------+----------------+------------------+-----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+-------+-------+\n",
      "|  Name| Age|Experience| Salary|Address|\n",
      "+------+----+----------+-------+-------+\n",
      "|surend|  20|         2|20000.0|     Tn|\n",
      "| kumar|  21|         1|30000.0|     Kl|\n",
      "|  hari|  23|         3|40000.0|     Tn|\n",
      "|sudhan|  20|         5|   NULL|     Kr|\n",
      "|  alex|NULL|         7|30000.0|     kl|\n",
      "|   bob|  25|      NULL|10000.0|   NULL|\n",
      "|  jhon|NULL|      NULL|   NULL|   NULL|\n",
      "+------+----+----------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop cloumns()\n",
    "dpcloumn=df_spark.drop('Adress')\n",
    "\n",
    "dpcloumn.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+-------+-------+\n",
      "|  Name|Age|Experience| Salary|Address|\n",
      "+------+---+----------+-------+-------+\n",
      "|surend| 20|         2|20000.0|     Tn|\n",
      "| kumar| 21|         1|30000.0|     Kl|\n",
      "|  hari| 23|         3|40000.0|     Tn|\n",
      "+------+---+----------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# null values colums drops\n",
    "df_spark.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+-------+-------+\n",
      "|  Name|Age|Experience| Salary|Address|\n",
      "+------+---+----------+-------+-------+\n",
      "|surend| 20|         2|20000.0|     Tn|\n",
      "| kumar| 21|         1|30000.0|     Kl|\n",
      "|  hari| 23|         3|40000.0|     Tn|\n",
      "+------+---+----------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop specific any (how)\n",
    "# how=all is to remove or print all values of Null\n",
    "# how=any is to remove or print all values any colums of Null\n",
    "df_spark.na.drop(how='any').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thersold for fillter Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+-------+-------+\n",
      "|  Name| Age|Experience| Salary|Address|\n",
      "+------+----+----------+-------+-------+\n",
      "|surend|  20|         2|20000.0|     Tn|\n",
      "| kumar|  21|         1|30000.0|     Kl|\n",
      "|  hari|  23|         3|40000.0|     Tn|\n",
      "|sudhan|  20|         5|   NULL|     Kr|\n",
      "|  alex|NULL|         7|30000.0|     kl|\n",
      "|   bob|  25|      NULL|10000.0|   NULL|\n",
      "+------+----+----------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# threshold \n",
    "# its remove to put the null values ex=2 any two null values columns is del\n",
    "df_spark.na.drop(how='any',thresh=3).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+-------+-------+\n",
      "|  Name| Age|Experience| Salary|Address|\n",
      "+------+----+----------+-------+-------+\n",
      "|surend|  20|         2|20000.0|     Tn|\n",
      "| kumar|  21|         1|30000.0|     Kl|\n",
      "|  hari|  23|         3|40000.0|     Tn|\n",
      "|sudhan|  20|         5|   NULL|     Kr|\n",
      "|  alex|NULL|         7|30000.0|     kl|\n",
      "+------+----+----------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# subset\n",
    "# to remove the columns to specific null values\n",
    "df_spark.na.drop(how='any',subset=['Experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+-------+-------+\n",
      "|  Name| Age|Experience| Salary|Address|\n",
      "+------+----+----------+-------+-------+\n",
      "|surend|  20|         2|20000.0|     Tn|\n",
      "| kumar|  21|         1|30000.0|     Kl|\n",
      "|  hari|  23|         3|40000.0|     Tn|\n",
      "|sudhan|  20|         5|   NULL|     Kr|\n",
      "|  alex|NULL|         7|30000.0|     kl|\n",
      "|   bob|  25|      NULL|10000.0|   NULL|\n",
      "|  jhon|NULL|      NULL|   NULL|   NULL|\n",
      "+------+----+----------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filling the Missing Value\n",
    "df_spark.na.fill(\"Missing Values\",\"Experience\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: double (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.dtypes\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputer Ml library Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "imputer=Imputer(\n",
    "    inputCols=['Age','Experience','Salary'],\n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['Age','Experience','Salary']],\n",
    ").setStrategy(\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+-------+-------+-----------+------------------+--------------+\n",
      "|  Name| Age|Experience| Salary|Address|Age_imputed|Experience_imputed|Salary_imputed|\n",
      "+------+----+----------+-------+-------+-----------+------------------+--------------+\n",
      "|surend|  20|         2|20000.0|     Tn|         20|                 2|       20000.0|\n",
      "| kumar|  21|         1|30000.0|     Kl|         21|                 1|       30000.0|\n",
      "|  hari|  23|         3|40000.0|     Tn|         23|                 3|       40000.0|\n",
      "|sudhan|  20|         5|   NULL|     Kr|         20|                 5|       30000.0|\n",
      "|  alex|NULL|         7|30000.0|     kl|         21|                 7|       30000.0|\n",
      "|   bob|  25|      NULL|10000.0|   NULL|         25|                 3|       10000.0|\n",
      "|  jhon|NULL|      NULL|   NULL|   NULL|         21|                 3|       30000.0|\n",
      "+------+----+----------+-------+-------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add imputation cols to df\n",
    "imputer.fit(df_spark).transform(df_spark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter Operrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+-------+-------+\n",
      "|  Name|Age|Experience| Salary|Address|\n",
      "+------+---+----------+-------+-------+\n",
      "|surend| 20|         2|20000.0|     Tn|\n",
      "|   bob| 25|      NULL|10000.0|   NULL|\n",
      "+------+---+----------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# salary of the people less then or equal to 20000\n",
    "df_spark.filter(\"Salary<=20000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  Name|Age|\n",
      "+------+---+\n",
      "|surend| 20|\n",
      "|   bob| 25|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.filter(\"Salary<=20000\").select([\"Name\",'Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+-------+-------+\n",
      "|  Name|Age|Experience| Salary|Address|\n",
      "+------+---+----------+-------+-------+\n",
      "|surend| 20|         2|20000.0|     Tn|\n",
      "|   bob| 25|      NULL|10000.0|   NULL|\n",
      "+------+---+----------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.filter(df_spark['Salary']<=20000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+-------+-------+\n",
      "|  Name|Age|Experience| Salary|Address|\n",
      "+------+---+----------+-------+-------+\n",
      "|surend| 20|         2|20000.0|     Tn|\n",
      "|   bob| 25|      NULL|10000.0|   NULL|\n",
      "+------+---+----------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.filter((df_spark['Salary']<=30000)&(df_spark['Salary']<=20000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+-------+-------+\n",
      "| Name| Age|Experience| Salary|Address|\n",
      "+-----+----+----------+-------+-------+\n",
      "|kumar|  21|         1|30000.0|     Kl|\n",
      "| hari|  23|         3|40000.0|     Tn|\n",
      "| alex|NULL|         7|30000.0|     kl|\n",
      "+-----+----+----------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.filter(~(df_spark['Salary']<=20000)).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
